{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff3IPRxSZltc",
        "outputId": "32e2824d-e246-492c-dd9d-c86c8cc4dfa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgbrDmfmM6I7",
        "outputId": "b66b0945-9f07-4f48-a8c8-9fa05885f93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrYj4WTHnESE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "start_time = time.time()\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import requests\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "LABELS_FILE_PATH = \"filteredFieldorNotLabels.csv\"\n",
        "IMAGE_FOLDER_PATH = \"filtered/\"\n",
        "RICE_FOLDER_PATH = \"Rice/\"\n",
        "\n",
        "imagesRoot = '/content/drive/MyDrive/GSV-CropType-Thailand/images/'\n",
        "\n",
        "def loadCSV(LABELS_FILE_PATH):\n",
        "    filenames = []\n",
        "    labels = []\n",
        "    with open(LABELS_FILE_PATH) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "        for row in csv_reader:\n",
        "            if line_count == 0:\n",
        "                print(f'Column names are {\", \".join(row)}')\n",
        "                line_count += 1\n",
        "            else:\n",
        "                filename = row[0]\n",
        "                idx = filename.find('-')\n",
        "                filenames.append(filename[idx+1:])\n",
        "                labels.append(row[1])\n",
        "                line_count += 1\n",
        "        print(f'Loaded CSV with {line_count} labels.')\n",
        "    return filenames, labels\n",
        "\n",
        "def encodeLabels(labels, classes):\n",
        "    classes = {'Field': 0, 'Not-Field': 1}\n",
        "    newLabels = []\n",
        "    for i, l in enumerate(labels):\n",
        "        if l not in '':\n",
        "            newLabels.append(classes[l])\n",
        "        else:\n",
        "            newLabels.append(2)\n",
        "            print(i)\n",
        "    return newLabels\n",
        "\n",
        "\n",
        "def getImagesFromFieldNotField(folderPath, batchSize):\n",
        "    data = []\n",
        "    fieldnames = os.listdir(folderPath+\"field/\")\n",
        "    nfieldnames = os.listdir(folderPath+\"notField/\")\n",
        "    print(\"LOADING\")\n",
        "    testTransform = transforms.Compose([\n",
        "      transforms.Resize((600, 600)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  ])\n",
        "    for fil in fieldnames:\n",
        "      try:\n",
        "        im = Image.open(folderPath+\"field/\"+fil)\n",
        "\n",
        "        image_array = np.array(im)\n",
        "\n",
        "        if image_array.shape[2] == 4:\n",
        "            image_array = image_array[:, :, :3]  # Remove the alpha channel if present\n",
        "        image = Image.fromarray(image_array)\n",
        "        cropped_image = image.resize((600, 600))\n",
        "      except:\n",
        "        continue\n",
        "      data.append([testTransform(cropped_image), 0])\n",
        "    for fil in fieldnames:\n",
        "      try:\n",
        "        im = Image.open(folderPath+\"notField/\"+fil)\n",
        "\n",
        "        image_array = np.array(im)\n",
        "        if image_array.shape[2] == 4:\n",
        "            image_array = image_array[:, :, :3]  # Remove the alpha channel if present\n",
        "        image = Image.fromarray(image_array)\n",
        "        cropped_image = image.resize((600, 600))\n",
        "      except:\n",
        "        continue\n",
        "      data.append([testTransform(cropped_image), 1])\n",
        "\n",
        "    print(\"Making splits\")\n",
        "    train, tes = torch.utils.data.random_split(data, [int(0.6*len(data)),  len(data)-int(0.6*len(data))])\n",
        "    test, val = torch.utils.data.random_split(tes, [int(0.5*len(tes)), len(tes)-int(0.5*len(tes))])\n",
        "\n",
        "\n",
        "    train = DataLoader(train, batch_size = batchSize, shuffle=True)\n",
        "    test = DataLoader(test, batch_size = batchSize, shuffle=True)\n",
        "    val = DataLoader(val, batch_size = batchSize, shuffle=True)\n",
        "    return train, test, val\n",
        "\n",
        "def uploadImages(folderPath, filenames, labels, batchSize):\n",
        "    data = []\n",
        "\n",
        "    labels = encodeLabels(labels)\n",
        "    fs = os.listdir(folderPath)\n",
        "    files = [f for f in fs]\n",
        "    print(files[0:10])\n",
        "    print(len(filenames))\n",
        "    j = 0\n",
        "    for i, fil in enumerate(filenames):\n",
        "\n",
        "        if fil[0:60] in files:\n",
        "          if labels[i] == 0 or labels[i] == 1:\n",
        "            j+=1\n",
        "            im = torchvision.io.read_image(folderPath + fil)\n",
        "            data.append([im.to(torch.float), labels[i]])\n",
        "\n",
        "    splitIdxval = round(len(data)*0.6)\n",
        "    splitIdxtest = round(len(data)*0.8)\n",
        "\n",
        "\n",
        "\n",
        "    train = DataLoader(data[:splitIdxval], batch_size = batchSize)\n",
        "    val = DataLoader(data[splitIdxval:splitIdxtest], batch_size = batchSize)\n",
        "    test = DataLoader(data[splitIdxtest:], batch_size = batchSize)\n",
        "\n",
        "\n",
        "    return train, val, test, j\n",
        "\n",
        "def train_imshow():\n",
        "    classes = {'Field': 0, 'Not-Field': 1}\n",
        "    dataiter = iter(trainloader)\n",
        "    images, labels = dataiter.next()\n",
        "    print(labels)\n",
        "    fig, axes = plt.subplots(figsize=(10, 4), ncols=5)\n",
        "    for i in range(5):\n",
        "        ax = axes[i]\n",
        "        ax.imshow(images[i].permute(1, 2, 0))\n",
        "        ax.title.set_text(' '.join('%5s' % classes[labels[i]]))\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv286jgUAfU3"
      },
      "outputs": [],
      "source": [
        "def getTrainIms(filenames, labels):\n",
        "  outFolder = \"fieldOrNot/\"\n",
        "  saveLoc = imagesRoot + outFolder\n",
        "  fieldDirList = os.listdir(saveLoc+\"field/\")\n",
        "  nfieldDirList = os.listdir(saveLoc+\"notField/\")\n",
        "\n",
        "  fields = []\n",
        "  nfields = []\n",
        "  fBearingPairs = {}\n",
        "  j = 0\n",
        "  for i, f in enumerate(filenames):\n",
        "\n",
        "    if f[29:32] != \"GSV\":\n",
        "      fBearingPairs, bearing = setBearing(f, filenames, fBearingPairs)\n",
        "      pano = f[7:29]\n",
        "      meta = getMeta(pano, bearing)\n",
        "      if meta != \"404\":\n",
        "        sLoc = saveLoc\n",
        "        if labels[i] == 'Field':\n",
        "          sLoc += 'field/'\n",
        "          dir = fieldDirList\n",
        "\n",
        "        elif labels[i] == 'Not-Field':\n",
        "          sLoc += 'notField/'\n",
        "          dir = nfieldDirList\n",
        "\n",
        "        getStreet(pano, sLoc, bearing, meta, dir)\n",
        "        j += 1\n",
        "\n",
        "  fof = 0\n",
        "  for i, f in enumerate(filenames):\n",
        "\n",
        "    if f[29:32] == \"GSV\":\n",
        "      if f[:29] in fBearingPairs:\n",
        "\n",
        "        bearing = getBearing(f, filenames, fBearingPairs)\n",
        "        pano = f[7:29]\n",
        "        meta = getMeta(pano, bearing)\n",
        "        if meta != \"404\":\n",
        "          sLoc = saveLoc\n",
        "          if labels[i] == 'Field':\n",
        "            sLoc += 'field/'\n",
        "            dir = fieldDirList\n",
        "\n",
        "          elif labels[i] == 'Not-Field':\n",
        "            sLoc += 'notField/'\n",
        "            dir = nfieldDirList\n",
        "\n",
        "          getStreet(pano, sLoc, bearing, meta, dir)\n",
        "          j +=1\n",
        "\n",
        "  print(\"total= \",  j)\n",
        "  print(\"fof=\", fof)\n",
        "\n",
        "def getBearing(f, filenames, fBearingPairs):\n",
        "\n",
        "  bearing = float(fBearingPairs[f[:29]]) + 180\n",
        "\n",
        "  return bearing%360\n",
        "\n",
        "def setBearing(f, filenames, fBearingPairs):\n",
        "  fbear = f[29:]\n",
        "  idx = fbear.find(\".\")\n",
        "  fbear = fbear[:idx+2]\n",
        "  fBearingPairs[f[:29]] = fbear\n",
        "  return fBearingPairs, fbear\n",
        "\n",
        "def getMeta(pano, bearing):\n",
        "  link = \"https://maps.googleapis.com/maps/api/streetview/metadata?size=640x640&pano=\"+str(pano)+\"&fov=80&heading=0&pitch=0&key=\" + KEY\n",
        "  res = requests.get(link)\n",
        "  resJson = res.json()\n",
        "  if resJson['status'] ==  'OK':\n",
        "    return resJson['date']+str(pano)+\"&bear\"+str(bearing) + \"&GSVLat\"+str(resJson[\"location\"][\"lat\"])+\"&GSVLon\"+str(resJson[\"location\"][\"lng\"])\n",
        "  else:\n",
        "    return \"404\"\n",
        "def getStreet(pano, SaveLoc, bearing, meta, dirlist):\n",
        "  link = \"https://maps.googleapis.com/maps/api/streetview?size=640x640&pano=\"+str(pano)+\"&fov=70&heading=\"+str(bearing)+\"&pitch=0&key=\" + KEY\n",
        "  fi = meta + \".jpg\"\n",
        "  if fi not in dirlist:\n",
        "    urllib.request.urlretrieve(link, os.path.join(SaveLoc,fi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1NiJKUbQma2",
        "outputId": "7b461d12-bb13-4bec-9559-998bfd797302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names are image, choice, created_at, annotation_id, id, lead_time, updated_at, annotator\n",
            "Loaded CSV with 815 labels.\n",
            "LOADING\n",
            "Making splits\n"
          ]
        }
      ],
      "source": [
        "\n",
        "BATCH_SIZE = 32\n",
        "filenames, labels = loadCSV(imagesRoot + \"fieldOrNotLabels-filtered-tight.csv\")\n",
        "trainloader, testloader, valloader = getImagesFromFieldNotField(imagesRoot + \"fieldOrNot/\", BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "VtX3H2EhnFP-",
        "outputId": "a45b0374-83e9-4b52-f80e-669effabf899"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-739aa1bea84c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Field Images : {}, Non-field Images : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mclassBalance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ],
      "source": [
        "def classBalance(labels):\n",
        "  # encodedLabels = encodeLabels(labels)\n",
        "  field = 0\n",
        "  nfield = 0\n",
        "  for l in labels:\n",
        "    if l == 0:\n",
        "      field +=1\n",
        "    elif l == 1:\n",
        "      nfield +=1\n",
        "\n",
        "  print('Field Images : {}, Non-field Images : {}'.format(field,nfield))\n",
        "\n",
        "classBalance(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHpXNf7KZjXl"
      },
      "outputs": [],
      "source": [
        "def make_train_step(model, optimizer, loss_fn):\n",
        "  def train_step(x,y):\n",
        "    #make prediction\n",
        "    yhat = model(x)\n",
        "    #enter train mode\n",
        "    model.train()\n",
        "    #compute loss\n",
        "    loss = loss_fn(yhat,y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    #optimizer.cleargrads()\n",
        "    yhatsig = torch.sigmoid(yhat)\n",
        "    acc = accuracy(yhatsig, y)\n",
        "\n",
        "    return loss, acc\n",
        "  return train_step\n",
        "\n",
        "def accuracy(preds, labels):\n",
        "    acc = 0\n",
        "    for i, pred in enumerate(preds):\n",
        "        p = torch.argmax(pred)\n",
        "\n",
        "        if torch.round(pred) == labels[i]:\n",
        "            acc +=1\n",
        "\n",
        "    return acc/len(preds)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "#add a new final layer\n",
        "nr_filters = model.fc.in_features  #number of input features of last layer\n",
        "model.fc = nn.Linear(nr_filters, 1)\n",
        "\n",
        "model = model.to(device)\n",
        "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "#loss\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "\n",
        "#optimizer\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "#train step\n",
        "train_step = make_train_step(model, optimizer, loss_fn)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "losses = []\n",
        "val_losses = []\n",
        "accs = []\n",
        "val_accs = []\n",
        "epoch_train_losses = []\n",
        "epoch_test_losses = []\n",
        "epoch_train_accs = []\n",
        "epoch_test_accs = []\n",
        "\n",
        "n_epochs = 20\n",
        "early_stopping_tolerance = 4\n",
        "early_stopping_threshold = 1.0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  print(epoch)\n",
        "  print(len(trainloader))\n",
        "  for i ,data in tqdm(enumerate(trainloader), total = len(trainloader)): #iterate ove batches\n",
        "    x_batch , y_batch = data\n",
        "\n",
        "    x_batch = x_batch.to(device) #move to gpu\n",
        "    y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "    y_batch = y_batch.to(device) #move to gpu\n",
        "\n",
        "\n",
        "    loss, acc = train_step(x_batch, y_batch)\n",
        "    epoch_acc += acc/len(trainloader)\n",
        "    epoch_loss += loss/len(trainloader)\n",
        "    losses.append(loss)\n",
        "    accs.append(acc)\n",
        "\n",
        "  epoch_train_losses.append(epoch_loss)\n",
        "  epoch_train_accs.append(epoch_acc)\n",
        "\n",
        "  print('\\nEpoch : {}, train loss : {}, train acc : {}'.format(epoch+1,epoch_loss, epoch_acc))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    cum_loss = 0\n",
        "    cum_acc = 0\n",
        "    for x_batch, y_batch in valloader:\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "      y_batch = y_batch.to(device)\n",
        "\n",
        "      #model to eval mode\n",
        "      model.eval()\n",
        "\n",
        "      yhat = model(x_batch)\n",
        "      yhatsig = torch.sigmoid(yhat)\n",
        "\n",
        "      val_loss = loss_fn(yhat,y_batch)\n",
        "      cum_loss += val_loss/len(valloader)\n",
        "      val_losses.append(val_loss.item())\n",
        "\n",
        "      val_acc = accuracy(yhatsig, y_batch)\n",
        "      cum_acc += val_acc/len(valloader)\n",
        "      val_accs.append(val_acc)\n",
        "\n",
        "\n",
        "    epoch_test_losses.append(cum_loss)\n",
        "    epoch_test_accs.append(cum_acc)\n",
        "\n",
        "    print('Epoch : {}, val loss : {} val acc: {}'.format(epoch+1,cum_loss, cum_acc))\n",
        "\n",
        "    best_loss = min(epoch_test_losses)\n",
        "    best_acc = max(epoch_test_accs)\n",
        "    #save best model\n",
        "    if cum_acc >= best_acc:\n",
        "      best_acc_model_wts = model.state_dict()\n",
        "    if cum_loss <= best_loss:\n",
        "      best_loss_model_wts = model.state_dict()\n",
        "\n",
        "    #early stopping\n",
        "    early_stopping_counter = 0\n",
        "    if cum_acc < best_acc:\n",
        "      early_stopping_counter +=1\n",
        "\n",
        "    if (early_stopping_counter == early_stopping_tolerance) or (best_acc >= early_stopping_threshold):\n",
        "      print(\"/nTerminating: early stopping\")\n",
        "      break #terminate training\n",
        "\n",
        "#load best model\n",
        "model.load_state_dict(best_acc_model_wts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST"
      ],
      "metadata": {
        "id": "cMDPasYZGWI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# print('Labels', np.array(labels).shape)\n",
        "\n",
        "\n",
        "print(np.array(testloader))\n",
        "\n",
        "def make_train_step(model, optimizer, loss_fn):\n",
        "  def train_step(x,y):\n",
        "    #make prediction\n",
        "    yhat = model(x)\n",
        "    #enter train mode\n",
        "    model.train()\n",
        "    #compute loss\n",
        "    loss = loss_fn(yhat,y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    #optimizer.cleargrads()\n",
        "    yhatsig = torch.sigmoid(yhat)\n",
        "    acc = accuracy(yhatsig, y)\n",
        "\n",
        "    return loss, acc\n",
        "  return train_step\n",
        "\n",
        "def accuracy(preds, labels):\n",
        "    acc = 0\n",
        "    for i, pred in enumerate(preds):\n",
        "        p = torch.argmax(pred)\n",
        "\n",
        "        if torch.round(pred) == labels[i]:\n",
        "            acc +=1\n",
        "\n",
        "    return acc/len(preds)\n",
        "\n",
        "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "#loss\n",
        "loss_fn = BCEWithLogitsLoss() #binary cross entropy with sigmoid, so no need to use sigmoid in the model\n",
        "\n",
        "#optimizer\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "#train step\n",
        "train_step = make_train_step(model, optimizer, loss_fn)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "losses = []\n",
        "val_losses = []\n",
        "accs = []\n",
        "val_accs = []\n",
        "epoch_train_losses = []\n",
        "epoch_test_losses = []\n",
        "epoch_train_accs = []\n",
        "epoch_test_accs = []\n",
        "\n",
        "n_epochs = 1\n",
        "early_stopping_tolerance = 4\n",
        "early_stopping_threshold = 1.0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for epoch in range(n_epochs):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  print(epoch)\n",
        "  print(len(trainloader))\n",
        "\n",
        "  #validation doesnt requires gradient\n",
        "  with torch.no_grad():\n",
        "    cum_loss = 0\n",
        "    cum_acc = 0\n",
        "\n",
        "    for x_batch, y_batch in testloader:\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "      y_batch = y_batch.to(device)\n",
        "\n",
        "      #model to eval mode\n",
        "      model.eval()\n",
        "\n",
        "      yhat = model(x_batch)\n",
        "      yhatsig = torch.sigmoid(yhat)\n",
        "      # for i in range(len(yhatsig)):\n",
        "      #   print(yhatsig[i], y_batch[i])\n",
        "      y_true.extend(y_batch.cpu().numpy())  # Append true labels\n",
        "      y_pred.extend(yhatsig.cpu().numpy())  # Append predicted labels\n",
        "      val_loss = loss_fn(yhat,y_batch)\n",
        "      cum_loss += val_loss/len(testloader)\n",
        "      val_losses.append(val_loss.item())\n",
        "\n",
        "      val_acc = accuracy(yhatsig, y_batch)\n",
        "      cum_acc += val_acc/len(testloader)\n",
        "      val_accs.append(val_acc)\n",
        "\n",
        "\n",
        "    epoch_test_losses.append(cum_loss)\n",
        "    epoch_test_accs.append(cum_acc)\n",
        "\n",
        "    print('Epoch : {}, test loss : {} test acc: {}'.format(epoch+1,cum_loss, cum_acc))\n",
        "    print(np.count_nonzero(np.array(y_pred) > 0.5))\n",
        "    print(y_true.count(1))\n",
        "    # print((np.array(y_pred)).astype(int))\n",
        "    precision = precision_score(y_true, (np.array(y_pred)).astype(int))\n",
        "    recall = recall_score(y_true, (np.array(y_pred)).astype(int))\n",
        "    print(\"Precision\", precision)\n",
        "    print(\"Recall\", recall)\n",
        "\n",
        "    best_loss = min(epoch_test_losses)\n",
        "    best_acc = max(epoch_test_accs)\n",
        "    #save best model\n",
        "    if cum_acc >= best_acc:\n",
        "      best_acc_model_wts = model.state_dict()\n",
        "    if cum_loss <= best_loss:\n",
        "      best_loss_model_wts = model.state_dict()\n",
        "\n",
        "    #early stopping\n",
        "    early_stopping_counter = 0\n",
        "    if cum_acc < best_acc:\n",
        "      early_stopping_counter +=1\n",
        "\n",
        "    if (early_stopping_counter == early_stopping_tolerance) or (best_acc >= early_stopping_threshold):\n",
        "      print(\"/nTerminating: early stopping\")\n",
        "      break #terminate training"
      ],
      "metadata": {
        "id": "fXt_PLMOGV0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fe0FT_747Qt"
      },
      "outputs": [],
      "source": [
        "def testModels(model, testloader):\n",
        "  with torch.no_grad():\n",
        "    cum_loss = 0\n",
        "    cum_acc = 0\n",
        "    for x_batch, y_batch in testloader:\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "      y_batch = y_batch.to(device)\n",
        "\n",
        "      #model to eval mode\n",
        "      model.eval()\n",
        "\n",
        "      yhat = model(x_batch)\n",
        "      yhatsig = torch.sigmoid(yhat)\n",
        "\n",
        "      test_loss = loss_fn(yhat,y_batch)\n",
        "      cum_loss += test_loss/len(testloader)\n",
        "\n",
        "      test_acc = accuracy(yhatsig, y_batch)\n",
        "      cum_acc += test_acc/len(testloader)\n",
        "\n",
        "    print('Test loss : {} test acc: {}'.format(cum_loss, cum_acc))\n",
        "\n",
        "# model.load_state_dict(best_acc_model_wts)\n",
        "testModels(model, testloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Section"
      ],
      "metadata": {
        "id": "8g25_9IPSBvq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq4oi3g3irOJ",
        "outputId": "98f10491-9ab5-4f01-ec27-9a611e249596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 178MB/s]\n"
          ]
        }
      ],
      "source": [
        "def saveModel(model, PATH):\n",
        "  torch.save(model.state_dict(), PATH)\n",
        "\n",
        "\n",
        "def loadModel(PATH):\n",
        "  # model = resnet18(pretrained=True, num_classes=4)  # where num_classes will be different\n",
        "\n",
        "  model = models.resnet18(pretrained=True)\n",
        "  nr_filters = model.fc.in_features\n",
        "  model.fc = nn.Linear(nr_filters, 1)\n",
        "  model.load_state_dict(torch.load(PATH))\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "PATH = imagesRoot + \"fieldOrNot-ResNet18-87%.pt\"\n",
        "model = loadModel(PATH)\n",
        "\n",
        "# saveModel(model, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMXFnHkZ1GGm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "folderPath = imagesRoot+\"Thailand16/\"\n",
        "filteredFilenames = os.listdir(folderPath)\n",
        "print(len(filteredFilenames))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3gFKb_HEbbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ac79d2-5e12-4c7a-8878-fbaa16005168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1036\n"
          ]
        }
      ],
      "source": [
        "outFolderPath = imagesRoot +'ThailandFieldOrNot/'\n",
        "classes = {0: 'field/', 1: 'notField/'}\n",
        "out = os.listdir(outFolderPath+'field/')\n",
        "print(len(out))\n",
        "out += os.listdir(outFolderPath+'notField/')\n",
        "\n",
        "allfiles = [x for x in filteredFilenames if x not in out]\n",
        "print(len(allfiles))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import concurrent.futures\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, folder_path, filenames):\n",
        "        self.folder_path = folder_path\n",
        "        self.filenames = [fn for fn in filenames if fn.lower().endswith('.jpg')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = os.path.join(self.folder_path, self.filenames[idx])\n",
        "        image = torchvision.io.read_image(file_path).to(torch.float)\n",
        "        return image, self.filenames[idx]\n",
        "\n",
        "def save_image(out_folder, class_folder, filename, image):\n",
        "    out_path = os.path.join(out_folder, class_folder, filename)\n",
        "    image = Image.fromarray(image)\n",
        "    image.save(out_path)\n",
        "\n",
        "def saveModelPreds(folderPath, outFolderPath, filenames, classes, numSaved=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    dataset = CustomDataset(folderPath, filenames)\n",
        "    imsLoader = DataLoader(dataset, batch_size=32)\n",
        "\n",
        "    model.eval()\n",
        "    outs = 0\n",
        "    model.to(device)\n",
        "    with torch.no_grad(), concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        for x_batch, fils in tqdm(imsLoader, total=len(imsLoader)):\n",
        "            x_batch = x_batch.to(device)\n",
        "            # print(x_batch.shape)\n",
        "            yhat = model(x_batch)\n",
        "            yhatsig = torch.sigmoid(yhat).cpu().numpy()\n",
        "            # print(np.rint(yhatsig[:, 0]).astype(int))\n",
        "            classFolders = [classes[int(index)] for index in np.rint(yhatsig[:, 0])]\n",
        "            im_batch = x_batch.permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "            for j in range(len(x_batch)):\n",
        "                future = save_image(outFolderPath, classFolders[j], fils[j], im_batch[j])\n",
        "                outs += 1\n",
        "\n",
        "    print(\"Images Classified:\", outs)\n",
        "\n",
        "outFolderPath = imagesRoot +'ThailandFieldOrNot/'\n",
        "folderPath = imagesRoot+\"Thailand15/\"\n",
        "\n",
        "classes = {0: 'field/', 1: 'notField/'}\n",
        "# saveModelPreds(folderPath, outFolderPath, filteredFilenames, classes)\n",
        "saveModelPreds(folderPath, outFolderPath, allfiles, classes)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc2o2X8BKyv6",
        "outputId": "90f0f6e4-7c33-4be9-a3a8-66b0360448e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [05:10<00:00,  6.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images Classified: 1518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}