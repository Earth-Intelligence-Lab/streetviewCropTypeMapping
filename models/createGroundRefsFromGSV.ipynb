{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I0SpAW6bzq6",
        "outputId": "d820868d-950f-43fc-b44f-1627d279841f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SywBjgO0b5z4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def count_images_by_year(root_dir, years=['2022', '2023']):\n",
        "    # Create a dictionary to hold the count of images per year\n",
        "    image_count = {year: 0 for year in years}\n",
        "    all_files = []\n",
        "    # Walk through the root directory\n",
        "    for subdir, dirs, files in os.walk(root_dir):\n",
        "        # Check if the subdir contains any of the years we're interested in\n",
        "        for year in years:\n",
        "            if year in subdir:\n",
        "                # Increment the count for the year by the number of image files\n",
        "                image_count[year] += len([file for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "                for file in files:\n",
        "                  all_files.append(file)\n",
        "    return image_count, all_files\n",
        "\n",
        "def load_classified_image_paths(csv_path):\n",
        "    if os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        return set(df['Filename'])\n",
        "    return set()\n",
        "\n",
        "# Define a Dataset for inference\n",
        "class InferenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, year, classified_image_paths, transform=None):\n",
        "        self.image_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root_dir)\n",
        "                            for f in filenames if str(year) in dp and f.lower().endswith(('.png', '.jpg', '.jpeg')) and os.path.join(dp, f) not in classified_image_paths]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, image_path\n",
        "\n",
        "\n",
        "def extract_patches(image, patch_size, stride):\n",
        "    patches = []\n",
        "    c, height, width = image.size()\n",
        "\n",
        "    for y in range(0, height - patch_size[1] + 1, stride):\n",
        "        for x in range(0, width - patch_size[0] + 1, stride):\n",
        "            patch = image[:, y:y + patch_size[1], x:x + patch_size[0]]\n",
        "            patches.append(patch)\n",
        "\n",
        "    return patches\n",
        "\n",
        "def classify_images_with_patches(data_loader, model, patch_size, stride, save_path, save_interval=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    batch_count = 0\n",
        "    with torch.no_grad():\n",
        "        for images, paths in tqdm(data_loader, desc='Classifying'):\n",
        "            images.to(device)\n",
        "            for image, path in zip(images, paths):\n",
        "                # Extract patches from the image\n",
        "                patches = extract_patches(image, patch_size, stride)\n",
        "                patches = torch.stack(patches).to(device)\n",
        "                # Classify each patch and aggregate the results\n",
        "                patch_outputs = model(patches)\n",
        "                logits = patch_outputs.logits\n",
        "                patch_predictions = torch.mean(logits, dim=0)\n",
        "\n",
        "                _, predicted = torch.max(patch_predictions, 0)\n",
        "                predictions.append((path, predicted.item()))\n",
        "\n",
        "            # Save intermediate results every 'save_interval' batches\n",
        "            batch_count += 1\n",
        "            if batch_count % save_interval == 0:\n",
        "                intermediate_df = pd.DataFrame(predictions, columns=['Filename', 'Class'])\n",
        "                if not os.path.isfile(save_path):\n",
        "                      # If not, write with header\n",
        "                      intermediate_df.to_csv(save_path, mode='w', index=False, header=True)\n",
        "                else:\n",
        "                      # If it exists, append without writing the header\n",
        "                      intermediate_df.to_csv(save_path, mode='a', index=False, header=False)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Define the custom transformation sequence\n",
        "class TopCropTransform:\n",
        "    \"\"\"Crops the top 30% of the image.\"\"\"\n",
        "    def __call__(self, img):\n",
        "        width, height = img.size\n",
        "        return img.crop((0, height * 0.3, width, height))\n",
        "\n",
        "transformations = transforms.Compose([\n",
        "    TopCropTransform(),                 # Custom crop\n",
        "    transforms.ToTensor(),              # Convert image to PyTorch tensor\n",
        "    transforms.Normalize(               # Normalize the image\n",
        "        mean=[0.485, 0.456, 0.406],     # Mean for each channel\n",
        "        std=[0.229, 0.224, 0.225]       # Standard deviation for each channel\n",
        "    )\n",
        "])\n",
        "\n",
        "\n",
        "imagesRoot = '/content/drive/MyDrive/GSV-CropType-Thailand/images/'\n",
        "root_dir_path = imagesRoot + 'ThailandFieldsByYear'\n",
        "\n",
        "image_counts, files = count_images_by_year(root_dir_path)\n",
        "\n",
        "for year, count in image_counts.items():\n",
        "    print(f\"Number of images in {year}: {count}\")\n",
        "\n",
        "df = pd.DataFrame(files, columns=['Filename'])\n",
        "print('Pre-Duplicates dropped', len(df))\n",
        "\n",
        "print('Post-Duplicates dropped', len(df.drop_duplicates()))\n",
        "unique = df.drop_duplicates()\n",
        "first_column = df.columns[0]\n",
        "\n",
        "print(df)\n",
        "# Check for duplicates\n",
        "duplicates = df.duplicated(subset=[first_column], keep=False)\n",
        "\n",
        "# Print duplicate rows\n",
        "print(\"Duplicate Rows based on the first column:\")\n",
        "print(df[duplicates])\n",
        "\n",
        "# Check for duplicates in the 'Filename' column\n",
        "duplicates = df[df.duplicated(subset=['Filename'], keep=False)]\n",
        "\n",
        "# Print the DataFrame with duplicates for reference\n",
        "print(\"DataFrame with Duplicates:\")\n",
        "print('DUPLICATES', duplicates)\n",
        "\n",
        "df = df.drop_duplictes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VULhsPt-3mLO"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTForImageClassification\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=5)\n",
        "\n",
        "model_state_dict = torch.load(imagesRoot+'ViT-Thailand-4cropsOther-lr2e-4-ep10_epoch_19.pth')\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "year_to_classify = 2022\n",
        "\n",
        "# results_csv_path = imagesRoot + 'ViTClassifiedImagesNoDup.csv'\n",
        "results_csv_path = imagesRoot + 'ViTClassifiedImages3.csv'\n",
        "df = pd.read_csv(results_csv_path)\n",
        "\n",
        "classified_image_paths = load_classified_image_paths(results_csv_path)\n",
        "# print(classified_image_paths)\n",
        "# classified_image_paths = dfnew['Filename'].tolist()\n",
        "\n",
        "inference_dataset = InferenceDataset(root_dir_path, year_to_classify, classified_image_paths, transform=transformations)\n",
        "\n",
        "print(len(inference_dataset.image_paths))\n",
        "# print(inference_dataset.image_paths[0]\n",
        "# Create a DataLoader for batch processing\n",
        "data_loader = DataLoader(inference_dataset, batch_size=32, num_workers=4)\n",
        "\n",
        "# Define patch size and stride (adjust these values as needed)\n",
        "patch_size = (224, 224)\n",
        "stride = 30\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "# Perform inference with patches\n",
        "predictions = classify_images_with_patches(data_loader, model, patch_size, stride, results_csv_path)\n",
        "\n",
        "\n",
        "results_csv_path = imagesRoot + 'ViTClassifiedImages3Post.csv'\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "df = pd.DataFrame(predictions, columns=['Filename', 'Class'])\n",
        "\n",
        "df.to_csv(results_csv_path, mode='a', index=False, header=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD1S3Eq6JSB4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def move_images(source_folder, target_root_folder, images_per_folder=500):\n",
        "    if not os.path.exists(source_folder):\n",
        "        print(f\"Source folder '{source_folder}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(target_root_folder):\n",
        "        print(f\"Target root folder '{target_root_folder}' does not exist.\")\n",
        "        return\n",
        "    filenames = []\n",
        "    image_files = [f for f in os.listdir(source_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    total_images = len(image_files)\n",
        "    folder_count = 0\n",
        "    image_count = 0\n",
        "\n",
        "    for i, image_file in enumerate(image_files):\n",
        "        if i % images_per_folder == 0:\n",
        "            folder_count += 1\n",
        "            target_folder = os.path.join(target_root_folder, f\"2022_10_{folder_count}\")\n",
        "            os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "        source_path = os.path.join(source_folder, image_file)\n",
        "        target_path = os.path.join(target_folder, image_file)\n",
        "        filenames.append(image_file)\n",
        "        shutil.move(source_path, target_path)\n",
        "\n",
        "        image_count += 1\n",
        "        if image_count >= images_per_folder:\n",
        "            image_count = 0\n",
        "\n",
        "    print(f\"Moved {total_images} images into {folder_count} folders.\")\n",
        "    return filenames\n",
        "source_folder = imagesRoot + 'ThailandFieldOrNot/field'\n",
        "target_root_folder = imagesRoot + 'ThailandFieldsByYear'\n",
        "filenames = move_images(source_folder, target_root_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVY9diSuPK6j"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "def extractParamFromFilenameAndFull(filename):\n",
        "  if filename.startswith('/content'):\n",
        "    parts = filename.split('/')\n",
        "    filename = parts[-1]\n",
        "\n",
        "    if filename[0:5] == '&date':\n",
        "\n",
        "      panoRe = r\"&panoid(.*?)&GSV\"\n",
        "      match = re.search(panoRe, filename)\n",
        "      if match:\n",
        "          date = filename[5:12]\n",
        "          panoId = match.group(1)\n",
        "          GSVLatIdx = filename.find('&GSVLat')\n",
        "          GSVLonIdx = filename.find('&GSVLon')\n",
        "          headIdx = filename.find('&head')\n",
        "          GSVLat =  filename[GSVLatIdx+7:GSVLonIdx]\n",
        "          GSVLon = filename[GSVLonIdx+7:headIdx]\n",
        "          headRe = r\"&head(.*?)&area\"\n",
        "          match = re.search(headRe, filename)\n",
        "          GSVLat = GSVLat\n",
        "          GSVLon = GSVLon\n",
        "\n",
        "          if match:\n",
        "            head = match.group(1)\n",
        "            return [filename, panoId, date, head, GSVLat, GSVLon]\n",
        "      else:\n",
        "         return [None, None, None,None, None, None]\n",
        "    else:\n",
        "\n",
        "      date = filename[0:7]\n",
        "      GSVLatIdx = filename.find('GSVLat')\n",
        "      GSVLonIdx = filename.find('GSVLon')\n",
        "\n",
        "      latIdx = filename.find('&lat')\n",
        "      lonIdx = filename.find('&lon')\n",
        "      endIdx = filename.find('.jpg')\n",
        "      lat = filename[latIdx+4:lonIdx]\n",
        "      lon = filename[lonIdx+4:endIdx]\n",
        "      GSVLat = filename[GSVLatIdx+6:GSVLonIdx]\n",
        "      GSVLon = filename[GSVLonIdx+6:latIdx-5]\n",
        "      head = filename[29:latIdx]\n",
        "      if GSVLatIdx > 0:\n",
        "        head = computeBearing( (float(GSVLat),float(GSVLon)) , (float(lat),float(lon)) )\n",
        "\n",
        "      GSVLat = GSVLat\n",
        "      GSVLon = GSVLon\n",
        "      panoId = filename[7:29]\n",
        "      return [filename, panoId, date, head, GSVLat, GSVLon]\n",
        "\n",
        "\n",
        "def save_to_csv(lons, lats, y_pred, folds, filenames, outfilename):\n",
        "    # Check if the file already exists\n",
        "    file_exists = os.path.isfile(outfilename)\n",
        "    fieldnames=['latitude','longitude','cropland_type', 'fold']\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    training_data = []\n",
        "    for i, lon in enumerate(lons):\n",
        "\n",
        "        if len(lon) >= 1:\n",
        "          # training_data.append({'latitude': lats[i][0], 'longitude': lons[i][0], 'cropland_type': y_pred[i], 'fold': folds[i] })\n",
        "          training_data.append((lats[i], lons[i], y_pred[i], folds[i] ))\n",
        "\n",
        "    with open(outfilename, 'a', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      if not file_exists:\n",
        "        writer.writerow(fieldnames)\n",
        "      writer.writerows(training_data)\n",
        "\n",
        "def generateGEEPoints(df):\n",
        "  labels = df.iloc[:, 1].tolist()\n",
        "  filenames = df.iloc[:, 0].tolist()\n",
        "  meta = [extractParamFromFilenameAndFull(filename) for filename in filenames]\n",
        "  STARTYEAR = 2022\n",
        "  ENDYEAR = 2022\n",
        "  STARTMONTH = 5\n",
        "  ENDMONTH = 10\n",
        "  lons = []\n",
        "  lats = []\n",
        "  y_pred = []\n",
        "  folds = []\n",
        "  heads = []\n",
        "  filenames_filtered = []\n",
        "  for i, point in enumerate(meta):\n",
        "    # print(point)\n",
        "    if withinDate(STARTMONTH, STARTYEAR, ENDMONTH, ENDYEAR, point[2]):\n",
        "      pt_lat, pt_lon = computePointOnField((float(point[-2]), float(point[-1])), float(point[-3]), 30)\n",
        "      # print(str(pt_lon))\n",
        "      lons.append(str(pt_lon))\n",
        "      lats.append(str(pt_lat))\n",
        "      y_pred.append(labels[i])\n",
        "      heads.append(str(point[-3]))\n",
        "      folds.append(0)\n",
        "      filenames_filtered.append(point[0])\n",
        "\n",
        "  print('Before', len(meta))\n",
        "  print('After', len(y_pred))\n",
        "  df = pd.DataFrame({'lon': lons, 'lat': lats, 'head': heads})\n",
        "  duplicates = df.duplicated(subset=['lon', 'lat'], keep='first')\n",
        "\n",
        "  # Print duplicate rows\n",
        "  print(\"Duplicate Rows based on the first column:\")\n",
        "  print(df[duplicates])\n",
        "  save_to_csv(lons, lats, y_pred, folds, filenames_filtered, imagesRoot + 'ViTClassifiedImagesProcClean2.csv')\n",
        "\n",
        "\n",
        "imagesRoot = '/content/drive/MyDrive/GSV-CropType-Thailand/images/'\n",
        "\n",
        "results_csv_path = imagesRoot + 'ViTClassifiedImages2Post.csv'\n",
        "\n",
        "df = pd.read_csv(results_csv_path)\n",
        "generateGEEPoints(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlv7lea8YqDt"
      },
      "outputs": [],
      "source": [
        "print(df.iloc[0,0][0:7])\n",
        "print(extractParamFromFilenameAndFull(df.iloc[0,0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir4Qe2N0ZF-3",
        "outputId": "c5edec49-26cb-4b9c-eb56-a0b62a1bd614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14.821481924251044, 101.82068200011909)\n",
            "(14.821555155418972, 101.82012899978739)\n"
          ]
        }
      ],
      "source": [
        "def computePointOnField(fro, theta, d):\n",
        "    import math\n",
        "    #calc distance of a point d away given point and bearing\n",
        "    R = 6371e3\n",
        "    #angular distance between start pt and destination pt\n",
        "    Ad = d/R\n",
        "    theta = math.radians(theta)\n",
        "    la1 = math.radians(fro[0])\n",
        "    lo1 = math.radians(fro[1])\n",
        "    #Inverse Haversine to calculate lat and lon coords of destination point\n",
        "    la2 =  math.asin(math.sin(la1) * math.cos(Ad) + math.cos(la1) * math.sin(Ad) * math.cos(theta))\n",
        "    lo2 = lo1 + math.atan2(math.sin(theta) * math.sin(Ad) * math.cos(la1) , math.cos(Ad) - math.sin(la1) * math.sin(la2))\n",
        "    return (math.degrees(la2),math.degrees(lo2))\n",
        "print(computePointOnField((14.82151854,\t101.8204055), 97.8, 30))\n",
        "print(computePointOnField((14.82151854,\t101.8204055), 97.8+180, 30))\n",
        "\n",
        "\n",
        "def withinDate(startMonth, startYear, endMonth, endYear, date):\n",
        "  year = int(date[0:4])\n",
        "  month = int(date[5:7])\n",
        "\n",
        "  if year > startYear and year < endYear:\n",
        "      return True\n",
        "  elif year == startYear and month >= startMonth:\n",
        "      return True\n",
        "  elif year == endYear and month <= endMonth:\n",
        "      return True\n",
        "  else:\n",
        "      return False\n",
        "def computeBearing(fro, to):\n",
        "    import math\n",
        "    # calculates distance in lat and lon btwn 2 pts and uses sin and cos to calculate components of direction vectors (y and x)\n",
        "    # then uses atan2 to calculate the angle between the direction vector and the x-axis\n",
        "    fro = (math.radians(fro[0]),\tmath.radians(fro[1]))\n",
        "    to = (math.radians(to[0]),\tmath.radians(to[1]))\n",
        "    y = math.sin(to[1]-fro[1]) * math.cos(to[0])\n",
        "    x = math.cos(fro[0])*math.sin(to[0]) - math.sin(fro[0])*math.cos(to[0])*math.cos(to[1]-fro[1])\n",
        "    θ = math.atan2(y, x)\n",
        "    brng = (θ*180/math.pi + 360) % 360\n",
        "    return brng\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EhI3i0hxri9",
        "outputId": "e439baf8-5fb7-48af-89b0-9f9f3d716e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file split into 8 parts and saved in 'splits' folder.\n"
          ]
        }
      ],
      "source": [
        "#Split csv file to avoid over memory errors in GEE\n",
        "import pandas as pd\n",
        "import os\n",
        "imagesRoot = '/content/drive/MyDrive/GSV-CropType-Thailand/images/'\n",
        "\n",
        "csv_file = imagesRoot + 'ViTClassifiedImagesProc.csv'\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "# Create a folder 'splits' to store the parts\n",
        "output_folder = 'preds8splits'\n",
        "# os.makedirs(imagesRoot + output_folder, exist_ok=True)\n",
        "\n",
        "# Calculate the size of each split\n",
        "split_size = len(data) // 8\n",
        "\n",
        "# Split and save\n",
        "for i in range(8):\n",
        "    start = i * split_size\n",
        "    end = None if i == 7 else (i + 1) * split_size  # Ensure the last split contains all remaining data\n",
        "    split_data = data.iloc[start:end]\n",
        "    split_data.to_csv(os.path.join(imagesRoot + output_folder, f'split_{i+1}.csv'), index=False)\n",
        "\n",
        "print(\"CSV file split into 8 parts and saved in 'splits' folder.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}